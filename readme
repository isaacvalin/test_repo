0. créer un environnement nnUnet : conda create -n nnUnet
conda activate nnUnet
installer la version de pytorch compatible avec CUDA (valider la version de CUDA : nvcc --version ) se fier à https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix pour la compatibilité 
installation pytorch ici : https://pytorch.org/get-started/locally/ 

1. conda activate l'environnement si déjà créé

2. cloner ce répertoire et s'assurer d'avoir la structure suivante : nnUnet-master -> dataset ; nnUNet
nnUnet correspond exactement au github de nnUNet (https://github.com/MIC-DKFZ/nnUNet/tree/master) RIEN N'EST À CHANGER DANS CE DOSSIER
dataset doit contenir les 3 dossiers suivants : nnUNet_raw ; nnUNet_preprocessed ; nnUNet_results
pour entrainer un modèle, nnUNet_preprocessed et nnUNet_results sont à laisser vide
nnUNet_raw doit contenir:
Dataset001_Brain/
├── dataset.json
├── imagesTr : contient/déposer les images d'entrainement
├── imagesTs  : contient/déposer les images pour faire l'inférence
└── labelsTr : contient-déposer les GT correspondant aux images d'entrainement

ATTENTION À LA NOMENCLATURE : LES IMAGES DOIEVENT ÊTRE NOMMÉES XXX_0000.nii.gz (ex : Volume_ID_ioasfjija_0000.nii.gz)
pour plus d'information, référez-vous à https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_format.md

2.5 Pour entrainer le modèle, 
nnUNetv2_train DATASET_NAME_OR_ID 2d FOLD [--npz]
nnUNetv2_train DATASET_NAME_OR_ID 3d_fullres FOLD [--npz]

3 inférence : pour faire de l'inférence sur un model pré-entrainé, assurez-vous d'avoir le dossier nnUNet_results du model entrainé, et qu'il contient les fichiers suivants :
nnUNet_results/
├── Dataset002_Heart
    │── nnUNetTrainer__nnUNetPlans__2d
    │    ├── fold_all 
    │    ├── dataset.json
    │    ├── dataset_fingerprint.json
    │    └── plans.json
    └── nnUNetTrainer__nnUNetPlans__3d_fullres
         ├── fold_all
         ├── dataset.json
         ├── dataset_fingerprint.json
         └── plans.json
ASSUREZ VOUS QUE LES DOSSIERS FOLD CONTIENNENT LES FICHIERS SUIVANTS: debug.json ; checkpoint_best.pth ; checkpoint_final.pth ; network_architecture.pdf ; progress.png ; validation_raw )
les fichiers checkpoint_best.pth ; checkpoint_final.pth sont à aller chercher sur le drive 

dans le terminal: cd jusqu'au dossier dataset/nnUNet_raw/Dataset001_Brain
s'assurer que environnement toujours activé

Dans le terminal, copier et coller les lignes suivantes après les avoir adaptées à votre espace de travail

export nnUNet_raw="chemin sur votre espace de travail/nnUnet_master/dataset/nnUNet_raw"
export nnUNet_preprocessed="chemin sur votre espace de travail/nnUnet_master/dataset/nnUNet_preprocessed"
export nnUNet_results="chemin sur votre espace de travail/nnUnet_master/dataset/nnUNet_results"

Nous sommes prêts à rouler l'inférence. Copier coller la ligne suivante dans le terminal
changer -o pour le chemin où vous voulez votre dossier output avec les prédictions
-c choisir entre 2d ou 3d_fullres en fonction du modèle pré-entrainé et vos besoins.
-t chemin d'accès vers le dossier results est optionnel, mais fut nécessaire dans mon cas pour une raison quelconque. NOTEZ bien la fin du chemin(/nnUnet_master/dataset/nnUNet_results/Dataset001_Brain/nnUNetTrainer) à garder tel quel ou si message d'erreur, s'assurer que le chemin mène vers les fichiers checkpoints (/nnUnet_master/dataset/nnUNet_results/Dataset001_Brain/nnUNetTrainer__nnUNetPlans_2d/fold_all/checkpoint_best.pth)
nnUNetv2_predict -i imagesTs/ -o /home/llg/Downloads/nnUnet_master/dataset/pred_nnUnet/ -d Dataset001_Brain -c 3d_fullres -f all -t /home/llg/Downloads/nnUnet_master/dataset/nnUNet_results/Dataset001_Brain/nnUNetTrainer

ex de commande terminal : (nnunet) llg@llg-ThinkPad-T15g-Gen-1-2:~/Downloads/nnUnet_master/dataset/nnUnet_raw/Dataset001_Brain$ nnUNetv2_predict -i imagesTs/ -o /home/llg/Downloads/nnUnet_master/dataset/pred_nnUnet/ -d Dataset001_Brain -c 2d -f all -t /home/llg/Downloads/nnUnet_master/dataset/nnUNet_results/Dataset001_Brain/nnUNetTrainer



